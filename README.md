# CloudGame
第四届全国高校云计算大赛项目中所用的代码

爬虫部分
对scrapy、requests以及urllib2.request进行简单总结

Requests:
功能比较全的一个第三方库，使用暂时没有发现什么不足，就是有那么一次使用数据丢失了，但我忘记是什么内容了

Urllib2.request
python自带的一个爬虫库，功能比较全，与requests差不多，但是当需要爬取的域名存在汉字时，需要做额外的处理
将汉字解析，才能继续进行。若是直接解析带有汉字的域名，爬虫报错

scrapy
集成度高，快。存数据库，存csv，存json都很方便。但缺点也是集成度太高，当出现bug时，很难跟踪调试。这次在爬取目录页+详情页格式的内容时，
出现数据丢失的现象，完全无法处理。
